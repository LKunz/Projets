{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week6/Text_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onO46LysT2dh"
   },
   "source": [
    "# Data Mining and Machine Learning - Week 6\n",
    "# Text Analytics\n",
    "\n",
    "[Text Analytics](https://people.ischool.berkeley.edu/~hearst/text-mining.html) (or text mining) is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" Written resources may include websites, books, emails, reviews, and articles.\n",
    "\n",
    "### Table of Contents\n",
    "#### 1. Summary\n",
    "* 1.1 Applications\n",
    "* 1.2 Tokenization and Stopwords\n",
    "* 1.3 Stemming and Lemmatization\n",
    "* 1.4 Text Representation\n",
    "\n",
    "#### 2. Text Preparation\n",
    "* 2.1 Install spaCy\n",
    "* 2.2 Tokenization\n",
    "* 2.3 Dependency Parsing\n",
    "* 2.4 Remove Stopwords\n",
    "* 2.5 Lemmatization\n",
    "* 2.6 Entity Detection\n",
    "* 2.7 Exercise\n",
    "* 2.8 Solution\n",
    "\n",
    "#### 3. Text Representation\n",
    "* 3.1 Bag of Words (BOW)\n",
    "* 3.2 TF-IDF Representation\n",
    "* 3.3 Exercise\n",
    "* 3.4 Solution\n",
    "\n",
    "#### 4. Text Classification: Alexa Reviews\n",
    "* 4.1 Load and prepare data\n",
    "* 4.2 Classification of the reviews using logistic regression\n",
    "* 4.3 How can we improve the accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0VsVUPLT4Tn"
   },
   "source": [
    "## 1. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHvrJj5yT8uQ"
   },
   "source": [
    "### 1.1 Applications\n",
    "There are many applications of text analytics, for example:\n",
    "* Search for relevant websites or articles using a search engine\n",
    "* Sentiment Analysis (e.g. classify tweets or film reviews as positive, neutral or negative)\n",
    "* Chatbots (e.g. Siri, Alexa)\n",
    "* Project idea: The Impact of Donald Trump’s Tweets on Financial Markets\n",
    "* Etc.\n",
    "\n",
    "### 1.2 Tokenization and Stopwords\n",
    "Tokens are the elementary building blocks (words, numbers, characters) in a document. Tokenization is the process of splitting an input\n",
    "sequence into tokens. Example: \"I love data science\" --> \"I\", \"love\", \"data\", \"science\". Stopwords are common words that appear very frequently (e.g. \"is\", \"and\", \"you\", etc.). It is convenient to remove them as they do not add much to the content of a document and are therefore generally not useful for text analysis or, worse still, make it worse by adding noise.\n",
    "\n",
    "### 1.3 Lemmatization and Stemming\n",
    "* Goal: have the same token for different forms of a word (e.g. fishing, fished, fisher, fishers, etc.)\n",
    "* Lemmatization: Find what is the lemma of a word (e.g. feet -> foot)\n",
    "* Stemming: one method for lemmatization where rules that remove the ending of a word are applied (e.g. fishing -> fish)\n",
    "\n",
    "\n",
    "### 1.4 Text Representation\n",
    "* Goal: transform text such that it can be used for text analysis\n",
    "* Bag of Words (BOW): works in many case but order is not preserved (solution: n-grams)\n",
    "* TF-IDF: emphasizes important words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXlVy05IUC-D"
   },
   "source": [
    "## 2. Text Preparation\n",
    "In this section, we explain how to prepare a text for analysis. This includes tockeninzing the text, removing stopwords, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA4tt9roTp8T"
   },
   "source": [
    "### 2.1 Install spaCy\n",
    "[spaCy](https://spacy.io/) is an open-source natural language processing library for Python. It is designed particularly for production use, and it can help us to build applications that process massive volumes of text efficiently.\n",
    "\n",
    "We install the library and its English-language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gf6I0JIYTp8U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\lucku\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB 5.7 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.6/12.8 MB 5.4 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.6/12.8 MB 5.4 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.6/12.8 MB 5.4 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.6/12.8 MB 5.4 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.6/12.8 MB 5.4 MB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.8/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.8/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.1/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     --- ------------------------------------ 1.1/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     --- ------------------------------------ 1.1/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     --- ------------------------------------ 1.1/12.8 MB 1.0 MB/s eta 0:00:12\n",
      "     --- ------------------------------------ 1.1/12.8 MB 1.0 MB/s eta 0:00:12\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.4 MB/s eta 0:00:08\n",
      "     ------- -------------------------------- 2.5/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.1/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.2/12.8 MB 2.8 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.3/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 4.9/12.8 MB 3.1 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.9/12.8 MB 3.5 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.7/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 4.0 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.7/12.8 MB 4.1 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.2/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 4.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.3/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.6/12.8 MB 5.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 9.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.7/12.8 MB 10.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 9.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.6.3)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucku\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full\n",
      "pipeline package name 'en_core_web_sm' instead.\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Install and update spaCy\n",
    "!pip install spacy\n",
    "\n",
    "# Download the english language model\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FhOeeRlqTp8Z"
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUibE-SyTp8d"
   },
   "source": [
    "### 2.2 Tokenization\n",
    "\n",
    "Tokenization is the process of breaking a text into pieces called tokens. A token simply refers to an individual part of a sentence having some semantic value. SpaCy‘s tokenizer takes input in form of unicode text and outputs a sequence of token objects. In addition, SpaCy automatically breaks your document into tokens when a document is created using the language model.\n",
    "\n",
    "Let’s take a look at a simple example. Imagine we have the following text, and we would like to tokenize it:\n",
    "\n",
    "> When learning data science, you shouldn't get discouraged!\n",
    "\n",
    "> Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\n",
    "\n",
    "There are a couple of different ways we can appoach this. The first is called __word tokenization__, which means breaking up the text into individual words. This is a critical step for many language processing applications, as they often require inputs in the form of individual words rather than longer strings of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Dj9OfB1BTp8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "When learning data science, you shouldn't get discouraged!\n",
       "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load English language model\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Declare text\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "# spaCy object is used to create a document\n",
    "my_doc = sp(text)\n",
    "\n",
    "my_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hc7fwTbPZdXJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a spaCy document\n",
    "type(my_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ex7-T-BkYXZM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'learning',\n",
       " 'data',\n",
       " 'science',\n",
       " ',',\n",
       " 'you',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'get',\n",
       " 'discouraged',\n",
       " '!',\n",
       " '\\n',\n",
       " 'Challenges',\n",
       " 'and',\n",
       " 'setbacks',\n",
       " 'are',\n",
       " \"n't\",\n",
       " 'failures',\n",
       " ',',\n",
       " 'they',\n",
       " \"'re\",\n",
       " 'just',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'journey',\n",
       " '.',\n",
       " 'You',\n",
       " \"'ve\",\n",
       " 'got',\n",
       " 'this',\n",
       " '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of tokens\n",
    "token_list = []\n",
    "\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xi29AcSsTp8j"
   },
   "source": [
    "As we can see, spaCy produces a list that contains each token as a separate item. Notice that it has recognized that contractions such as _shouldn’t_ actually represent two distinct words, and has thus broken them down into two distinct tokens.\n",
    "\n",
    "In the example above, we first load the language dictionary. Here we load the english one using `spacy.load('en_core_web_sm')` create an object of this class, \"sp\", which is used to create documents with linguistic annotations and various language properties. After creating the document, we create a list of tokens.\n",
    "\n",
    "We can also see the parts-of-speech (POS) of each of these tokens using the `.pos_` attribute, as shown below. POS tagging can be really useful, particularly if you have words or tokens that can have multiple POS tags. For instance, the word \"fish\" can be used as both a noun and verb, depending upon the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NvkGwDJoWoIs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When SCONJ\n",
      "learning VERB\n",
      "data NOUN\n",
      "science NOUN\n",
      ", PUNCT\n",
      "you PRON\n",
      "should AUX\n",
      "n't PART\n",
      "get AUX\n",
      "discouraged VERB\n",
      "! PUNCT\n",
      "\n",
      " SPACE\n",
      "Challenges NOUN\n",
      "and CCONJ\n",
      "setbacks NOUN\n",
      "are AUX\n",
      "n't PART\n",
      "failures NOUN\n",
      ", PUNCT\n",
      "they PRON\n",
      "'re AUX\n",
      "just ADV\n",
      "part NOUN\n",
      "of ADP\n",
      "the DET\n",
      "journey NOUN\n",
      ". PUNCT\n",
      "You PRON\n",
      "'ve AUX\n",
      "got VERB\n",
      "this PRON\n",
      "! PUNCT\n"
     ]
    }
   ],
   "source": [
    "# POS\n",
    "for word in my_doc:\n",
    "    print(word.text, word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cUsKHgp7Y3yM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "like VERB\n",
      "to PART\n",
      "fish VERB\n",
      "-----------------\n",
      "I PRON\n",
      "eat VERB\n",
      "a DET\n",
      "fish NOUN\n"
     ]
    }
   ],
   "source": [
    "# Another example\n",
    "doc1 = sp(\"I like to fish\") # verb\n",
    "doc2 = sp(\"I eat a fish\") # noun\n",
    "\n",
    "for word in doc1:\n",
    "  print(word.text, word.pos_)\n",
    "\n",
    "print(\"-----------------\")\n",
    "\n",
    "for word in doc2:\n",
    "  print(word.text, word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCiU1-PDXKpp"
   },
   "source": [
    "\n",
    "If we want, we can also break the text into sentences rather than words. This is called __sentence tokenization__. When performing sentence tokenization, the tokenizer looks for specific characters that normally fall between sentences, like periods, exclaimation points, and newline characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "odi_WkNZTp8k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When learning data science, you shouldn't get discouraged!\\n\",\n",
       " \"Challenges and setbacks aren't failures, they're just part of the journey.\",\n",
       " \"You've got this!\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of sentence tokens\n",
    "sents_list = []\n",
    "\n",
    "for sent in my_doc.sents:\n",
    "    sents_list.append(sent.text)\n",
    "\n",
    "sents_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te403rkXbGKf"
   },
   "source": [
    "### 2.3 Dependency Parsing\n",
    "__Depenency parsing__ is a language processing technique that allows to better determine the meaning of a sentence by analyzing how it is constructed to determine how the individual words relate to each other.\n",
    "\n",
    "Consider, for example, the sentence “Joe throws the ball.” We have two nouns (Joe and ball) and one verb (throws). But we can’t just look at these words individually, or we may end up thinking that the ball throws Joe! To understand the sentence correctly, we need to look at the word order and sentence structure, not just the words.\n",
    "\n",
    "Below, we have a short sentence. We’ll use a spaCy method called `noun_chunks`, which breaks the input down into nouns and the words describing them, and iterate through each chunk in our source text, identifying the word, its root, its dependency identification, and which chunk it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YFLButK_bOor"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Joe Joe nsubj threw\n",
      "a ball ball dobj threw\n",
      "President Donald Donald nsubj hit\n",
      "pursuit pursuit pobj in\n",
      "the ball ball pobj of\n",
      "a wall wall dobj hit\n"
     ]
    }
   ],
   "source": [
    "doc = sp(\" Joe threw a ball, and President Donald, in pursuit of the ball, hit a wall.\") # notice the space at the beginning\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7VmqTQQ9bYTl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"852b7fed8d414a54b6bc0c2b8afaa52d-0\" class=\"displacy\" width=\"1970\" height=\"437.0\" direction=\"ltr\" style=\"max-width: none; height: 437.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\"> </tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">SPACE</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">Joe</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">threw</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">ball,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">President</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">Donald,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">pursuit</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1370\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1370\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1490\">ball,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1490\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1610\">hit</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1610\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1730\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1730\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1850\">wall.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1850\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-0\" stroke-width=\"2px\" d=\"M70,302.0 C70,242.0 150.0,242.0 150.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,304.0 L62,292.0 78,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-1\" stroke-width=\"2px\" d=\"M190,302.0 C190,242.0 270.0,242.0 270.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M190,304.0 L182,292.0 198,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-2\" stroke-width=\"2px\" d=\"M430,302.0 C430,242.0 510.0,242.0 510.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M430,304.0 L422,292.0 438,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-3\" stroke-width=\"2px\" d=\"M310,302.0 C310,182.0 515.0,182.0 515.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M515.0,304.0 L523.0,292.0 507.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-4\" stroke-width=\"2px\" d=\"M310,302.0 C310,122.0 640.0,122.0 640.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M640.0,304.0 L648.0,292.0 632.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-5\" stroke-width=\"2px\" d=\"M790,302.0 C790,242.0 870.0,242.0 870.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,304.0 L782,292.0 798,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-6\" stroke-width=\"2px\" d=\"M910,302.0 C910,62.0 1605.0,62.0 1605.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910,304.0 L902,292.0 918,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-7\" stroke-width=\"2px\" d=\"M1030,302.0 C1030,122.0 1600.0,122.0 1600.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1030,304.0 L1022,292.0 1038,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-8\" stroke-width=\"2px\" d=\"M1030,302.0 C1030,242.0 1110.0,242.0 1110.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1110.0,304.0 L1118.0,292.0 1102.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-9\" stroke-width=\"2px\" d=\"M1150,302.0 C1150,242.0 1230.0,242.0 1230.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1230.0,304.0 L1238.0,292.0 1222.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-10\" stroke-width=\"2px\" d=\"M1390,302.0 C1390,242.0 1470.0,242.0 1470.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1390,304.0 L1382,292.0 1398,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-11\" stroke-width=\"2px\" d=\"M1270,302.0 C1270,182.0 1475.0,182.0 1475.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1475.0,304.0 L1483.0,292.0 1467.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-12\" stroke-width=\"2px\" d=\"M310,302.0 C310,2.0 1610.0,2.0 1610.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1610.0,304.0 L1618.0,292.0 1602.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-13\" stroke-width=\"2px\" d=\"M1750,302.0 C1750,242.0 1830.0,242.0 1830.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1750,304.0 L1742,292.0 1758,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-14\" stroke-width=\"2px\" d=\"M1630,302.0 C1630,182.0 1835.0,182.0 1835.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-852b7fed8d414a54b6bc0c2b8afaa52d-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1835.0,304.0 L1843.0,292.0 1827.0,292.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's visualize this\n",
    "displacy.render(doc, style=\"dep\", jupyter= True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_Z6MiCtTp8n"
   },
   "source": [
    "### 2.4 Remove Stopwords\n",
    "Most text data that we work with is going to contain a lot of words that are not actually useful to us. These words, called stopwords, are useful in human speech, but they do not have much to contribute to data analysis. Removing stopwords helps us eliminate noise and distraction from our text data, and also speeds up the time analysis takes (since there are fewer words to process). This makes text analysis more efficient.\n",
    "\n",
    "Let’s take a look at the stopwords spaCy includes by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_kB8kI0eTp8o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 326\n",
      "20 stopwords: ['then', 'while', 'than', 'of', 'could', 'do', 'through', '’d', 'most', 'seems', 'a', 'that', 'although', 'thru', 'never', 'none', \"n't\", 'get', 'itself', \"'m\"]\n"
     ]
    }
   ],
   "source": [
    "# Import stopwords from English language\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Print total number of stopwords\n",
    "print('Number of stopwords: %d' % len(spacy_stopwords))\n",
    "\n",
    "# Print 20 stopwords\n",
    "print('20 stopwords: %s' % list(spacy_stopwords)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aw0Wfvu4Tp8q"
   },
   "source": [
    "Now that we’ve got our list of stopwords, let’s use it to remove the stopwords from the text string we were working on in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Eq5yvoZtlhsd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "When learning data science, you shouldn't get discouraged!\n",
       "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words will be removed?\n",
    "my_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "y54Kiz9zTp8r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning',\n",
       " 'data',\n",
       " 'science',\n",
       " ',',\n",
       " 'discouraged',\n",
       " '!',\n",
       " '\\n',\n",
       " 'Challenges',\n",
       " 'setbacks',\n",
       " 'failures',\n",
       " ',',\n",
       " 'journey',\n",
       " '.',\n",
       " 'got',\n",
       " '!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare list for filtered sentence\n",
    "filtered_sent = []\n",
    "\n",
    "# Filter stopwords\n",
    "for word in my_doc:\n",
    "    if word.is_stop == False:\n",
    "        filtered_sent.append(word.text)\n",
    "\n",
    "filtered_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UOvM2NFdl29W"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " ',',\n",
       " 'you',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'get',\n",
       " '!',\n",
       " '\\n',\n",
       " 'and',\n",
       " 'are',\n",
       " \"n't\",\n",
       " ',',\n",
       " 'they',\n",
       " \"'re\",\n",
       " 'just',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " '.',\n",
       " 'You',\n",
       " \"'ve\",\n",
       " 'this',\n",
       " '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also remove the punctuation\n",
    "filtered_sent2 = []\n",
    "removed_tokens = []\n",
    "\n",
    "# Filter stopwords, punctuation and spaces\n",
    "for word in my_doc:\n",
    "  if (word.is_stop == True) or (word.is_punct == True) or (word.is_space == True):\n",
    "    removed_tokens.append(word.text)\n",
    "  else:\n",
    "    filtered_sent2.append(word.text)\n",
    "\n",
    "removed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mw-ebvZom1EP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning',\n",
       " 'data',\n",
       " 'science',\n",
       " 'discouraged',\n",
       " 'Challenges',\n",
       " 'setbacks',\n",
       " 'failures',\n",
       " 'journey',\n",
       " 'got']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sent2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imbw_TUkTp8v"
   },
   "source": [
    "### 2.5 Lemmatization\n",
    "Lemmatization is a way of dealing with the fact that while words like connect, connection, connecting, connected, etc. aren’t exactly the same, they all have the same essential meaning: connect. The differences in spelling have grammatical functions in spoken language, but for machine processing, those differences can be confusing, so we need a way to change all the words that are forms of the word connect into the word connect itself.\n",
    "\n",
    "One method for doing this is called __stemming__. Stemming involves simply lopping off easily-identified prefixes and suffixes to produce what’s often the simplest version of a word, the root. Connection, for example, would have the -ion suffix removed and be correctly reduced to connect. This kind of simple stemming is often all that’s needed, but lemmatization—which actually looks at words and their roots (called lemma) as described in the dictionary—is more precise (e.g feet -> foot).\n",
    "\n",
    "Let's look at this simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UTqSImYfTp8v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run run\n",
      "runs run\n",
      "ran run\n",
      "running run\n",
      "runner runner\n",
      "runners runner\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lem = sp(\"run runs ran running runner runners\")\n",
    "\n",
    "# Find lemma for each word\n",
    "for word in lem:\n",
    "    print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7J5Na4DTp84"
   },
   "source": [
    "### 2.6 Entity Detection\n",
    "\n",
    "__Entity detection__, also called entity recognition, is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within a text. This is really helpful for quickly extracting information from the text, since you can quickly pick out important topics or indentify key sections of it.\n",
    "\n",
    "Let’s try out some entity detection using a few paragraphs from this [article](https://www.bloomberg.com/features/trump-tweets-market/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nvlb9S71Tp84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Donald Trump, 'PERSON', 380),\n",
       " (Twitter, 'PERSON', 380),\n",
       " (American, 'NORP', 381),\n",
       " (U.S., 'GPE', 384),\n",
       " (late January, 'DATE', 391),\n",
       " (American, 'NORP', 381),\n",
       " (more than 10, 'CARDINAL', 397),\n",
       " (that month alone, 'DATE', 391)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = sp(\"\"\"\n",
    "President Donald Trump gets a lot of attention for using Twitter to attack American trading partners, political foes, and media companies. But he often takes to the platform to celebrate the strength of the world’s largest economy and its publicly-traded companies.\n",
    "\n",
    "Before U.S. stocks peaked in late January, he drew a direct connection between the increase in market value of American companies and his administration’s pro-growth policies on more than 10 occasions in that month alone.\n",
    "\"\"\")\n",
    "\n",
    "entities = [(i, i.label_, i.label) for i in article.ents]\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1ScYkq7Tp87"
   },
   "source": [
    "The above example how spaCy is able to identify a variety of different entity types, including specific locations (GPE), date-related words (DATE), important numbers (CARDINAL), specific individuals (PERSON), etc.\n",
    "\n",
    "Using `displaCy` we can also visualize the text, with each identified entity highlighted by a color and labeled. We’ll use `style=\"ent\"` to tell displaCy that we want to visualize entities here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hGDOBT6zTp88"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>President \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Donald Trump\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " gets a lot of attention for using \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Twitter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " to attack \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " trading partners, political foes, and media companies. But he often takes to the platform to celebrate the strength of the world’s largest economy and its publicly-traded companies.<br><br>Before \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.S.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " stocks peaked in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    late January\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", he drew a direct connection between the increase in market value of \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " companies and his administration’s pro-growth policies on \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    more than 10\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " occasions in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    that month alone\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".<br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(article, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Erd3X6Q-n1_Y"
   },
   "source": [
    "### 2.7 Exercise\n",
    "For each word in the sentence below, print its lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zyJOSspCn7W6"
   },
   "outputs": [],
   "source": [
    "sentence = \"\"\"The happiness of your life depends upon the quality of your thoughts: therefore, guard accordingly, and take care that you entertain no notions unsuitable to virtue and reasonable nature.\"\"\"\n",
    "\n",
    "# Print lemma\n",
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRR_jyLPp8Q2"
   },
   "source": [
    "Create two lists, the first one containing the punctuation and the second one the words (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "fVwmiV9_n7SU"
   },
   "outputs": [],
   "source": [
    "# [YOUR CODE HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKE3SNk6aHvL"
   },
   "source": [
    "### 2.8 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gDVhIdRwaHAG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the\n",
      "happiness happiness\n",
      "of of\n",
      "your your\n",
      "life life\n",
      "depends depend\n",
      "upon upon\n",
      "the the\n",
      "quality quality\n",
      "of of\n",
      "your your\n",
      "thoughts thought\n",
      ": :\n",
      "therefore therefore\n",
      ", ,\n",
      "guard guard\n",
      "accordingly accordingly\n",
      ", ,\n",
      "and and\n",
      "take take\n",
      "care care\n",
      "that that\n",
      "you you\n",
      "entertain entertain\n",
      "no no\n",
      "notions notion\n",
      "unsuitable unsuitable\n",
      "to to\n",
      "virtue virtue\n",
      "and and\n",
      "reasonable reasonable\n",
      "nature nature\n",
      ". .\n",
      "[:, ,, ,, .]\n",
      "[The, happiness, of, your, life, depends, upon, the, quality, of, your, thoughts, therefore, guard, accordingly, and, take, care, that, you, entertain, no, notions, unsuitable, to, virtue, and, reasonable, nature]\n"
     ]
    }
   ],
   "source": [
    "# Lemma\n",
    "doc = sp(sentence)\n",
    "for word in doc:\n",
    "  print(word.text, word.lemma_)\n",
    "\n",
    "# punct and tokens\n",
    "punct = []\n",
    "tokens = []\n",
    "for word in doc:\n",
    "  if word.is_punct == True:\n",
    "    punct.append(word)\n",
    "  elif word.is_space == False:\n",
    "    tokens.append(word)\n",
    "\n",
    "print(punct)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QZOutDwsYij"
   },
   "source": [
    "## 3. Text Representation\n",
    "We now show how to transform a text into an usable input for text classification. We use the first sentence of the article from the last section and two other sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "X2MjeqoQxdOx"
   },
   "outputs": [],
   "source": [
    "# Article as a string, not a spacy object\n",
    "article = \"\"\"\n",
    "President Donald Trump gets a lot of attention for using Twitter to attack American trading partners, political foes, and media companies.\"\"\"\n",
    "\n",
    "# Sentences\n",
    "s1 = \"\"\"Donald Trump is a great friend, and he has four or five Picassos on his plane. And that's where I would look at them.\"\"\" # from Shaquille O'Neal\n",
    "s2 = \"\"\"Donald Trump is a phony, a fraud. His promises are as worthless as a degree from Trump University.\"\"\" # from Mitt Romney\n",
    "\n",
    "# List of sentences\n",
    "texts = [article, s1, s2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCFQrtHLtMTt"
   },
   "source": [
    "### 3.1 Bag of Words (BOW)\n",
    "We use the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class of scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZyzBm0BUtLc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0,\n",
       "        0, 1, 1, 0, 0, 1, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using default tokenizer \n",
    "count = CountVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
    "bow = count.fit_transform(texts)\n",
    "\n",
    "# Show feature matrix\n",
    "bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GRmUUc0kzi3_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['american', 'american trading', 'attack', 'attack american',\n",
       "       'attention', 'attention using', 'companies', 'degree',\n",
       "       'degree trump', 'donald', 'donald trump', 'foes', 'foes media',\n",
       "       'fraud', 'fraud promises', 'friend', 'friend picassos', 'gets',\n",
       "       'gets lot', 'great', 'great friend', 'look', 'lot',\n",
       "       'lot attention', 'media', 'media companies', 'partners',\n",
       "       'partners political', 'phony', 'phony fraud', 'picassos',\n",
       "       'picassos plane', 'plane', 'plane look', 'political',\n",
       "       'political foes', 'president', 'president donald', 'promises',\n",
       "       'promises worthless', 'trading', 'trading partners', 'trump',\n",
       "       'trump gets', 'trump great', 'trump phony', 'trump university',\n",
       "       'twitter', 'twitter attack', 'university', 'using',\n",
       "       'using twitter', 'worthless', 'worthless degree'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get feature names\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "# View feature names\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "sxCSvGUTz2tl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>american</th>\n",
       "      <th>american trading</th>\n",
       "      <th>attack</th>\n",
       "      <th>attack american</th>\n",
       "      <th>attention</th>\n",
       "      <th>attention using</th>\n",
       "      <th>companies</th>\n",
       "      <th>degree</th>\n",
       "      <th>degree trump</th>\n",
       "      <th>donald</th>\n",
       "      <th>...</th>\n",
       "      <th>trump great</th>\n",
       "      <th>trump phony</th>\n",
       "      <th>trump university</th>\n",
       "      <th>twitter</th>\n",
       "      <th>twitter attack</th>\n",
       "      <th>university</th>\n",
       "      <th>using</th>\n",
       "      <th>using twitter</th>\n",
       "      <th>worthless</th>\n",
       "      <th>worthless degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   american  american trading  attack  attack american  attention  \\\n",
       "0         1                 1       1                1          1   \n",
       "1         0                 0       0                0          0   \n",
       "2         0                 0       0                0          0   \n",
       "\n",
       "   attention using  companies  degree  degree trump  donald  ...  trump great  \\\n",
       "0                1          1       0             0       1  ...            0   \n",
       "1                0          0       0             0       1  ...            1   \n",
       "2                0          0       1             1       1  ...            0   \n",
       "\n",
       "   trump phony  trump university  twitter  twitter attack  university  using  \\\n",
       "0            0                 0        1               1           0      1   \n",
       "1            0                 0        0               0           0      0   \n",
       "2            1                 1        0               0           1      0   \n",
       "\n",
       "   using twitter  worthless  worthless degree  \n",
       "0              1          0                 0  \n",
       "1              0          0                 0  \n",
       "2              0          1                 1  \n",
       "\n",
       "[3 rows x 54 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show as a dataframe\n",
    "pd.DataFrame(\n",
    "    bow.todense(), \n",
    "    columns=feature_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7tNGrTutQ2r"
   },
   "source": [
    "### 3.2 TF-IDF Representation\n",
    "\n",
    "\n",
    "Recall that:\n",
    "\n",
    "- term frequency tf = count(word, document) / len(document) \n",
    "- term frequency idf = log( len(collection) / count(document_containing_term, collection) )\n",
    "- tf-idf = tf * idf \n",
    "\n",
    "It is important to mention that the IDF value for a word remains the same throughout all the documents as it depends upon the total number of documents. On the other hand, TF values of a word differ from document to document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "djkUI6hXtWlr"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>american</th>\n",
       "      <th>attack</th>\n",
       "      <th>attention</th>\n",
       "      <th>companies</th>\n",
       "      <th>degree</th>\n",
       "      <th>donald</th>\n",
       "      <th>foes</th>\n",
       "      <th>fraud</th>\n",
       "      <th>friend</th>\n",
       "      <th>gets</th>\n",
       "      <th>...</th>\n",
       "      <th>plane</th>\n",
       "      <th>political</th>\n",
       "      <th>president</th>\n",
       "      <th>promises</th>\n",
       "      <th>trading</th>\n",
       "      <th>trump</th>\n",
       "      <th>twitter</th>\n",
       "      <th>university</th>\n",
       "      <th>using</th>\n",
       "      <th>worthless</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.260841</td>\n",
       "      <td>0.260841</td>\n",
       "      <td>0.260841</td>\n",
       "      <td>0.260841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154057</td>\n",
       "      <td>0.260841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.260841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.260841</td>\n",
       "      <td>0.260841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260841</td>\n",
       "      <td>0.154057</td>\n",
       "      <td>0.260841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260841</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.41894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.41894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359347</td>\n",
       "      <td>0.212236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359347</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   american    attack  attention  companies    degree    donald      foes  \\\n",
       "0  0.260841  0.260841   0.260841   0.260841  0.000000  0.154057  0.260841   \n",
       "1  0.000000  0.000000   0.000000   0.000000  0.000000  0.247433  0.000000   \n",
       "2  0.000000  0.000000   0.000000   0.000000  0.359347  0.212236  0.000000   \n",
       "\n",
       "      fraud   friend      gets  ...    plane  political  president  promises  \\\n",
       "0  0.000000  0.00000  0.260841  ...  0.00000   0.260841   0.260841  0.000000   \n",
       "1  0.000000  0.41894  0.000000  ...  0.41894   0.000000   0.000000  0.000000   \n",
       "2  0.359347  0.00000  0.000000  ...  0.00000   0.000000   0.000000  0.359347   \n",
       "\n",
       "    trading     trump   twitter  university     using  worthless  \n",
       "0  0.260841  0.154057  0.260841    0.000000  0.260841   0.000000  \n",
       "1  0.000000  0.247433  0.000000    0.000000  0.000000   0.000000  \n",
       "2  0.000000  0.424472  0.000000    0.359347  0.000000   0.359347  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using default tokenizer in TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
    "features = tfidf.fit_transform(texts)\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "AcyNjGlfHjlo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nPresident Donald Trump gets a lot of attention for using Twitter to attack American trading partners, political foes, and media companies.',\n",
       " \"Donald Trump is a great friend, and he has four or five Picassos on his plane. And that's where I would look at them.\",\n",
       " 'Donald Trump is a phony, a fraud. His promises are as worthless as a degree from Trump University.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcrRL3rRqgBy"
   },
   "source": [
    "### 3.3 Exercise\n",
    "Create a TF-IDF Representation of the three above sentences using bigrams and the following stopwords: [\"and\", \"a\", \"is\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "fzo8uviDrTLm"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYgMIUcVbCC6"
   },
   "source": [
    "### 3.4 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HEUWiXjubBn_"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>american trading</th>\n",
       "      <th>are as</th>\n",
       "      <th>as degree</th>\n",
       "      <th>as worthless</th>\n",
       "      <th>at them</th>\n",
       "      <th>attack american</th>\n",
       "      <th>attention for</th>\n",
       "      <th>degree from</th>\n",
       "      <th>donald trump</th>\n",
       "      <th>five picassos</th>\n",
       "      <th>...</th>\n",
       "      <th>trading partners</th>\n",
       "      <th>trump gets</th>\n",
       "      <th>trump great</th>\n",
       "      <th>trump phony</th>\n",
       "      <th>trump university</th>\n",
       "      <th>twitter to</th>\n",
       "      <th>using twitter</th>\n",
       "      <th>where would</th>\n",
       "      <th>worthless as</th>\n",
       "      <th>would look</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.240085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240085</td>\n",
       "      <td>0.240085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240085</td>\n",
       "      <td>0.240085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240085</td>\n",
       "      <td>0.240085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141798</td>\n",
       "      <td>0.240085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284569</td>\n",
       "      <td>0.284569</td>\n",
       "      <td>0.284569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284569</td>\n",
       "      <td>0.168071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284569</td>\n",
       "      <td>0.284569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284569</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   american trading    are as  as degree  as worthless   at them  \\\n",
       "0          0.240085  0.000000   0.000000      0.000000  0.000000   \n",
       "1          0.000000  0.000000   0.000000      0.000000  0.240085   \n",
       "2          0.000000  0.284569   0.284569      0.284569  0.000000   \n",
       "\n",
       "   attack american  attention for  degree from  donald trump  five picassos  \\\n",
       "0         0.240085       0.240085     0.000000      0.141798       0.000000   \n",
       "1         0.000000       0.000000     0.000000      0.141798       0.240085   \n",
       "2         0.000000       0.000000     0.284569      0.168071       0.000000   \n",
       "\n",
       "   ...  trading partners  trump gets  trump great  trump phony  \\\n",
       "0  ...          0.240085    0.240085     0.000000     0.000000   \n",
       "1  ...          0.000000    0.000000     0.240085     0.000000   \n",
       "2  ...          0.000000    0.000000     0.000000     0.284569   \n",
       "\n",
       "   trump university  twitter to  using twitter  where would  worthless as  \\\n",
       "0          0.000000    0.240085       0.240085     0.000000      0.000000   \n",
       "1          0.000000    0.000000       0.000000     0.240085      0.000000   \n",
       "2          0.284569    0.000000       0.000000     0.000000      0.284569   \n",
       "\n",
       "   would look  \n",
       "0    0.000000  \n",
       "1    0.240085  \n",
       "2    0.000000  \n",
       "\n",
       "[3 rows x 47 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(2, 2), stop_words=[\"and\", \"a\", \"is\"])\n",
    "features = tfidf.fit_transform(texts)\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARu2ONJb06Sr"
   },
   "source": [
    "## 4. Text Classification: Alexa reviews\n",
    "\n",
    "We are going to use a real-world data set: [Amazon Alexa product reviews](https://www.kaggle.com/bittlingmayer/amazonreviews).\n",
    "\n",
    "This data set comes as a tab-separated file (.tsv). It has has five columns: `rating`, `date`, `variation`, `verified_reviews`, `feedback`.\n",
    "\n",
    "`rating` denotes the rating each user gave Alexa (out of 5). `date` indicates the date of the review, and `variation` describes which model the user reviewed. `verified_reviews` contains the text of each review, and `feedback` contains a sentiment label, with 1 denoting positive sentiment (the user liked it) and 0 denoting negative sentiment (the user didn’t).\n",
    "\n",
    "We are going develop a classification model that looks at the review text and predicts whether a review is positive or negative. Since this data set already includes whether a review is positive or negative in the `feedback` column, we can use those answers to train and test our model. Our goal here is to produce an accurate model that we could then use to process new user reviews and quickly determine whether they were positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzfnR4ua2uLw"
   },
   "source": [
    "### 4.1 Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "5rS4hAiA05wQ"
   },
   "outputs": [],
   "source": [
    "# Import additional packages\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Sxrlx4eD0Ygk"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>5</td>\n",
       "      <td>17-May-18</td>\n",
       "      <td>Black</td>\n",
       "      <td>Amazingly fun. My daughter and I just asked qu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>Works great, love the fact you can play the sa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>5</td>\n",
       "      <td>29-Jun-18</td>\n",
       "      <td>Black  Plus</td>\n",
       "      <td>We have a great time using this as a family. E...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Configuration: Fire TV Stick</td>\n",
       "      <td>Worked as advertised !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>4</td>\n",
       "      <td>22-Jul-18</td>\n",
       "      <td>Black  Plus</td>\n",
       "      <td>I love it but does not play well with my Amazo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>4</td>\n",
       "      <td>29-Jul-18</td>\n",
       "      <td>White  Spot</td>\n",
       "      <td>so far just ok</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>5</td>\n",
       "      <td>26-Jul-18</td>\n",
       "      <td>White  Plus</td>\n",
       "      <td>Great device, my grandkids had a ball setting ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>4</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Small speaker tin sound. Great with good Bluet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>5</td>\n",
       "      <td>30-Jul-18</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Excellent. Bought another one yesterday!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>5</td>\n",
       "      <td>29-Jul-18</td>\n",
       "      <td>Black  Spot</td>\n",
       "      <td>Hands free control</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating       date                     variation  \\\n",
       "692        5  17-May-18                         Black   \n",
       "3102       5  30-Jul-18                    White  Dot   \n",
       "2096       5  29-Jun-18                   Black  Plus   \n",
       "2263       5  30-Jul-18  Configuration: Fire TV Stick   \n",
       "1988       4  22-Jul-18                   Black  Plus   \n",
       "1164       4  29-Jul-18                   White  Spot   \n",
       "1930       5  26-Jul-18                   White  Plus   \n",
       "2995       4  30-Jul-18                    Black  Dot   \n",
       "2902       5  30-Jul-18                    Black  Dot   \n",
       "1130       5  29-Jul-18                   Black  Spot   \n",
       "\n",
       "                                       verified_reviews  feedback  \n",
       "692   Amazingly fun. My daughter and I just asked qu...         1  \n",
       "3102  Works great, love the fact you can play the sa...         1  \n",
       "2096  We have a great time using this as a family. E...         1  \n",
       "2263                             Worked as advertised !         1  \n",
       "1988  I love it but does not play well with my Amazo...         1  \n",
       "1164                                     so far just ok         1  \n",
       "1930  Great device, my grandkids had a ball setting ...         1  \n",
       "2995  Small speaker tin sound. Great with good Bluet...         1  \n",
       "2902           Excellent. Bought another one yesterday!         1  \n",
       "1130                                 Hands free control         1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "url = \"https://raw.githubusercontent.com/LKunz/Projets/refs/heads/main/Data%20Science/data/amazon_alexa.tsv\"\n",
    "df = pd.read_csv(url, delimiter=\"\\t\")\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "WNjV_B2E3nLd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3150 entries, 0 to 3149\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   rating            3150 non-null   int64 \n",
      " 1   date              3150 non-null   object\n",
      " 2   variation         3150 non-null   object\n",
      " 3   verified_reviews  3150 non-null   object\n",
      " 4   feedback          3150 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 123.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "P6hCSlbq30UT"
   },
   "outputs": [],
   "source": [
    "# Change date to datetime\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dP8sYT2y34CU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3150 entries, 0 to 3149\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   rating            3150 non-null   int64         \n",
      " 1   date              3150 non-null   datetime64[ns]\n",
      " 2   variation         3150 non-null   object        \n",
      " 3   verified_reviews  3150 non-null   object        \n",
      " 4   feedback          3150 non-null   int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 123.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yK8u3mNw35HC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2893\n",
       "0     257\n",
       "Name: feedback, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base rate: the data-set is unbalanced!\n",
    "df.feedback.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4d4NPeOq4A0X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9184"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(df.feedback.value_counts()[1] / len(df), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iFd7_hl4eNJ"
   },
   "source": [
    "###### Tokening the Data With spaCy\n",
    "\n",
    "We create a `spacy_tokenizer()` function that accepts a sentence as input and processes the sentence into tokens, performing lemmatization, lowercasing, and removing stopwords.\n",
    "\n",
    "__A note from spacy documentation__: spaCy adds a special case for pronouns: all pronouns are lemmatized to the special token `-PRON-`. Unlike verbs and common nouns, there’s no clear base form of a personal pronoun. Should the lemma of “me” be “I”, or should we normalize person as well, giving “it” — or maybe “he”? spaCy’s solution is to introduce a novel symbol, `-PRON-`, which is used as the lemma for all personal pronouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "hrHhCTFN5ZXw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "QCVM6xVQ5fP2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['then',\n",
       " 'while',\n",
       " 'than',\n",
       " 'of',\n",
       " 'could',\n",
       " 'do',\n",
       " 'through',\n",
       " '’d',\n",
       " 'most',\n",
       " 'seems']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of stopwords\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "list(stop_words)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "-_ALYORw4LN0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We were pleasantly surprised at the sound quality and the many features. How nice to easily hear a particular artist or hear our books just by a simple command. Looking forward to exploring other options with this device.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load English language model\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Create token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = sp(sentence)\n",
    "\n",
    "    # Lemmatize each token and convert each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Remove stop words and punctuation\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # Return preprocessed list of tokens\n",
    "    return mytokens\n",
    "\n",
    "# Example\n",
    "review = df[\"verified_reviews\"].sample()\n",
    "review.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "dlOwdF_16JV-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pleasantly',\n",
       " 'surprised',\n",
       " 'sound',\n",
       " 'quality',\n",
       " 'feature',\n",
       " 'nice',\n",
       " 'easily',\n",
       " 'hear',\n",
       " 'particular',\n",
       " 'artist',\n",
       " 'hear',\n",
       " 'book',\n",
       " 'simple',\n",
       " 'command',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'explore',\n",
       " 'option',\n",
       " 'device']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenizer(review.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ5Yl8uR9eYv"
   },
   "source": [
    "#### Vectorization Feature Engineering (TF-IDF)\n",
    "\n",
    "We use the TF-IDF (Term Frequency-Inverse Document Frequency) to vectorize the documents. This is a way of representing how important a particular term is in the context of a given document, based on how many times the term appears and how many other documents that same term appears in. The higher the TF-IDF, the more important that term is to that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "0-874sxt8iNM"
   },
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer) # we use the above defined tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jO56WUV9yXn"
   },
   "source": [
    "### 4.2 Classification of the reviews using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "L7JkcGTb9ty9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2088    I really like this item. I already had the ech...\n",
       "287     Technology in such a small device. The price w...\n",
       "516           Pleased with the order just what I wanted!!\n",
       "832                                               Love it\n",
       "890                                   Works as advertised\n",
       "                              ...                        \n",
       "664     Great for mom its inside an owlhead in her jac...\n",
       "3125    This product is easy to use and very entertain...\n",
       "1318    You're going to have to shell out an extra $20...\n",
       "723     Fun item to play with and get used to using.  ...\n",
       "2863                                        Great product\n",
       "Name: verified_reviews, Length: 2520, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features\n",
    "X = df['verified_reviews'] # the features we want to analyze\n",
    "ylabels = df['feedback'] # the labels, or answers, we want to test against\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=1234)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "P6dbSYsF-HRi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2088    1\n",
       "287     1\n",
       "516     1\n",
       "832     1\n",
       "890     1\n",
       "       ..\n",
       "664     1\n",
       "3125    1\n",
       "1318    1\n",
       "723     1\n",
       "2863    1\n",
       "Name: feedback, Length: 2520, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "dB8PkfsP-JYi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(tokenizer=&lt;function spacy_tokenizer at 0x000001CDE0D208B0&gt;)),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(tokenizer=&lt;function spacy_tokenizer at 0x000001CDE0D208B0&gt;)),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(tokenizer=&lt;function spacy_tokenizer at 0x000001CDE0D208B0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(tokenizer=<function spacy_tokenizer at 0x000001CDE0D208B0>)),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Fit model on training set\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "e07kHgHb-hC2"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate(true, pred):\n",
    "    precision = precision_score(true, pred)\n",
    "    recall = recall_score(true, pred)\n",
    "    f1 = f1_score(true, pred)\n",
    "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(true, pred)}\")\n",
    "    print(f\"ACCURACY SCORE:\\n{accuracy_score(true, pred):.4f}\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "_PjJWqHp_HvQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      "[[  2  49]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9222\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9220\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9594\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation - test set\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRrITD4y_nbb"
   },
   "source": [
    "For the test set, the model correctly identifies a sentiment 92.22% of the time. This is only slightly better than the base rate (91.84%). The difference lies in the two cases that the model correctly classifies as negative. Therefore, the model does not work very well. This may be due to the fact that we have an unbalanced sample with too much positive reviews. Maybe the model cannot learn how to classify negative reviews well since there are too few examples of them.\n",
    "\n",
    "A recall of 1 means that if a sentiment is positive, it will be classififed as positive.\n",
    "\n",
    "We observe approximately the same on the training set, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "AeB_oSPKP7vZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      "[[   6  200]\n",
      " [   0 2314]]\n",
      "ACCURACY SCORE:\n",
      "0.9206\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9204\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9586\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on training set\n",
    "evaluate(y_train, pipe.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "6ZAzALsgXu_v"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491                            I've already returned it.\n",
       "381    It worked for a month or so then it stopped. I...\n",
       "Name: verified_reviews, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which reviews are classified as negative in test set?\n",
    "X_test.loc[pipe.predict(X_test) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "lBMomG3RYeo2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Can't turn of &#34;things to try&#34; on the screen scroll. This  is a big promotion device for Amazon.  Packing it up and sending it back.  They really want to get in our heads.  Enough already.\",\n",
       "       \"This worked well for about 6 months but then stopped connecting to the internet (wifi) and none of the trouble shooting efforts worked.  Amazon customer service couldn't help further and told me warranty was only 3 months, which was very disappointing, so they weren't willing to replace.\",\n",
       "       'product stopped working after return time ran out',\n",
       "       'Item has never worked. Out of box it is broken. Spent several days trying to get it working and running same &#34;fixes&#34; from Amazon. The only thing accomplished is I will never order another refurbished device.',\n",
       "       'Terrible. Stopped working after one day.',\n",
       "       \"This worked well for about 6 months but then stopped connecting to the internet (wifi) and none of the trouble shooting efforts worked.  Amazon customer service couldn't help further and told me warranty was only 3 months, which was very disappointing, so they weren't willing to replace.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which reviews are classified as negaive in training set?\n",
    "X_train.loc[pipe.predict(X_train) == 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "9SnrtzOhS9cP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I really love the product. It is very helpful....\n",
       "1              It stopped working, I want to return it\n",
       "2                           I don't like it, it is bad\n",
       "dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction for new reviews\n",
    "example_review_1 = \"I really love the product. It is very helpful. I use it everyday\" # positive\n",
    "example_review_2 = \"It stopped working, I want to return it\" # negative\n",
    "example_review_3 = \"I don't like it, it is bad\" # negative\n",
    "\n",
    "examples = pd.Series([example_review_1, example_review_2, example_review_3])\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Yrnvd-ZPT7aR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHne-tgCScWk"
   },
   "source": [
    "### 4.3 How can we improve the accuracy?\n",
    "In order to improve the prediction, we can try to:\n",
    "* Resample our data (i.e. add negative examples for better training)\n",
    "* Tun the hyperparameters of the model\n",
    "* Improve text preparation\n",
    "* Use another classififer\n",
    "\n",
    "We illustrate how to improve text preparation, resampling, and the use of another classifier below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyEyaUltsGNB"
   },
   "source": [
    "#### 4.3.1 Improve text preparation\n",
    "The purpose here is to optimize the parameters of the [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "u0KXBFJyuKWn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 1), 1, 1.0, 'word'],\n",
       " [(1, 1), 1, 1.0, 'char'],\n",
       " [(1, 2), 1, 1.0, 'word'],\n",
       " [(1, 2), 1, 1.0, 'char'],\n",
       " [(1, 3), 1, 1.0, 'word'],\n",
       " [(1, 3), 1, 1.0, 'char'],\n",
       " [(2, 2), 1, 1.0, 'word'],\n",
       " [(2, 2), 1, 1.0, 'char'],\n",
       " [(2, 3), 1, 1.0, 'word'],\n",
       " [(2, 3), 1, 1.0, 'char']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of configs\n",
    "def configs():\n",
    "\n",
    "    models = list()\n",
    "    \n",
    "    # Define config lists\n",
    "    ngram_range = [(1,1), (1,2), (1, 3), (2, 2), (2, 3), (3, 3)]\n",
    "    min_df = [1]\n",
    "    max_df = [1.0]\n",
    "    analyzer=['word', 'char']\n",
    "    \n",
    "    # Create config instances\n",
    "    for n in ngram_range:\n",
    "        for i in min_df:\n",
    "            for j in max_df:\n",
    "              for a in analyzer:\n",
    "                    cfg = [n, i, j, a]\n",
    "                    models.append(cfg)\n",
    "    return models\n",
    "\n",
    "configs = configs()\n",
    "configs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zypqialXQZTZ",
    "outputId": "3b12abf3-cb8b-452f-fbd3-44b32d7383e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:  [(1, 1), 1, 1.0, 'word']\n",
      "CONFUSION MATRIX:\n",
      "[[  2  49]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9222\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9220\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9594\n",
      "-----------------------\n",
      "CONFIG:  [(1, 1), 1, 1.0, 'char']\n",
      "CONFUSION MATRIX:\n",
      "[[  0  51]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9190\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9190\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9578\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n",
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:  [(1, 2), 1, 1.0, 'word']\n",
      "CONFUSION MATRIX:\n",
      "[[  0  51]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9190\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9190\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9578\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:  [(1, 2), 1, 1.0, 'char']\n",
      "CONFUSION MATRIX:\n",
      "[[  0  51]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9190\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9190\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9578\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:  [(1, 3), 1, 1.0, 'word']\n",
      "CONFUSION MATRIX:\n",
      "[[  0  51]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9190\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9190\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9578\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:  [(1, 3), 1, 1.0, 'char']\n",
      "CONFUSION MATRIX:\n",
      "[[  0  51]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9190\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9190\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9578\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:  [(2, 2), 1, 1.0, 'word']\n",
      "CONFUSION MATRIX:\n",
      "[[  0  51]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9190\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9190\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9578\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:  [(2, 2), 1, 1.0, 'char']\n",
      "CONFUSION MATRIX:\n",
      "[[  1  50]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9206\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9205\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9586\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:  [(2, 3), 1, 1.0, 'word']\n",
      "CONFUSION MATRIX:\n",
      "[[  0  51]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9190\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9190\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9578\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:  [(2, 3), 1, 1.0, 'char']\n",
      "CONFUSION MATRIX:\n",
      "[[  1  50]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9206\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9205\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9586\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:  [(3, 3), 1, 1.0, 'word']\n",
      "CONFUSION MATRIX:\n",
      "[[  0  51]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9190\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9190\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9578\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:  [(3, 3), 1, 1.0, 'char']\n",
      "CONFUSION MATRIX:\n",
      "[[  0  51]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9190\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9190\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9578\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# Define list for result\n",
    "result = []\n",
    "\n",
    "for config in configs:\n",
    "\n",
    "    # Redefine vectorizer\n",
    "    tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer, \n",
    "                                   ngram_range=config[0],\n",
    "                                   min_df=config[1], max_df=config[2], analyzer=config[3])\n",
    "\n",
    "    # Define classifier\n",
    "    classifier = LogisticRegression()\n",
    "\n",
    "    # Create pipeline\n",
    "    pipe = Pipeline([('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "    # Fit model on training set\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    # Print accuracy on test set\n",
    "    print(\"CONFIG: \", config)\n",
    "    evaluate(y_test, y_pred)\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "    # Append to result\n",
    "    result.append([config, accuracy_score(y_test, y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Boqm0jsz4CvZ"
   },
   "source": [
    "Our tries do not work, we have to try further or use something else to improve prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pygcSii_4SF4"
   },
   "source": [
    "#### 4.3.2 Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "L27GTYuFNRJU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3012</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>I am spoiled by one downstairs...so got this o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2920</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Cool little speaker..I love her alarms and con...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1413</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>White  Show</td>\n",
       "      <td>I like the echo show its the best! big screen ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1173</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-28</td>\n",
       "      <td>Black  Spot</td>\n",
       "      <td>Love these!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1808</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-29</td>\n",
       "      <td>White  Plus</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>3047</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>Echo Dot responds to us when we aren't even ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>3048</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>NOT CONNECTED TO MY PHONE PLAYLIST :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>3067</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>The only negative we have on this product is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>3091</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>I didn’t order it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>3096</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>White  Dot</td>\n",
       "      <td>The product sounded the same as the emoji spea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>514 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  rating       date    variation  \\\n",
       "0     3012       5 2018-07-30   White  Dot   \n",
       "1     2920       5 2018-07-30   Black  Dot   \n",
       "2     1413       5 2018-07-30  White  Show   \n",
       "3     1173       5 2018-07-28  Black  Spot   \n",
       "4     1808       5 2018-07-29  White  Plus   \n",
       "..     ...     ...        ...          ...   \n",
       "509   3047       1 2018-07-30   Black  Dot   \n",
       "510   3048       1 2018-07-30   White  Dot   \n",
       "511   3067       2 2018-07-30   Black  Dot   \n",
       "512   3091       1 2018-07-30   Black  Dot   \n",
       "513   3096       1 2018-07-30   White  Dot   \n",
       "\n",
       "                                      verified_reviews  feedback  \n",
       "0    I am spoiled by one downstairs...so got this o...         1  \n",
       "1    Cool little speaker..I love her alarms and con...         1  \n",
       "2    I like the echo show its the best! big screen ...         1  \n",
       "3                                          Love these!         1  \n",
       "4                                                              1  \n",
       "..                                                 ...       ...  \n",
       "509  Echo Dot responds to us when we aren't even ta...         0  \n",
       "510              NOT CONNECTED TO MY PHONE PLAYLIST :(         0  \n",
       "511  The only negative we have on this product is t...         0  \n",
       "512                                  I didn’t order it         0  \n",
       "513  The product sounded the same as the emoji spea...         0  \n",
       "\n",
       "[514 rows x 6 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create balanced dataframe - base rate = 0.5\n",
    "df_new = pd.concat([df[df[\"feedback\"] == 1].sample(len(df[df[\"feedback\"] == 0])), df[df[\"feedback\"] == 0]], axis=0).reset_index()\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "pzVuGW63NRF3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      "[[47  9]\n",
      " [10 37]]\n",
      "ACCURACY SCORE:\n",
      "0.8155\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.8043\n",
      "\tRecall: 0.7872\n",
      "\tF1_Score: 0.7957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Select features\n",
    "X = df_new['verified_reviews'] # the features we want to analyze\n",
    "ylabels = df_new['feedback'] # the labels, or answers, we want to test against\n",
    "\n",
    "# Train test split\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X, ylabels, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Define classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Fit model on training set\n",
    "pipe.fit(X_train_b, y_train_b)\n",
    "\n",
    "# Predictions\n",
    "y_pred_b = pipe.predict(X_test_b)\n",
    "\n",
    "# Evaluation - test set\n",
    "evaluate(y_test_b, y_pred_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBKezH2Ctl3y"
   },
   "source": [
    "#### 4.3.3 Use another classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "JRze7VxPzZ14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      "[[ 10  41]\n",
      " [  0 579]]\n",
      "ACCURACY SCORE:\n",
      "0.9349\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9339\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9658\n",
      "CONFUSION MATRIX:\n",
      "[[ 189   17]\n",
      " [   0 2314]]\n",
      "ACCURACY SCORE:\n",
      "0.9933\n",
      "CLASSIFICATION REPORT:\n",
      "\tPrecision: 0.9927\n",
      "\tRecall: 1.0000\n",
      "\tF1_Score: 0.9963\n"
     ]
    }
   ],
   "source": [
    "# Use random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define vectorizer\n",
    "tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer) # we use the above defined tokenizer\n",
    "\n",
    "# Define classifier\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Fit model on training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation - test set\n",
    "evaluate(y_test, y_pred)\n",
    "\n",
    "# Evaluation - training set\n",
    "evaluate(y_train, pipe.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2I-HjNh5aTF"
   },
   "source": [
    "Of course, combining the three above-mentioned techniques should give the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Io3YM1jV1YN3"
   },
   "source": [
    "#### BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "L9D5xSc5_y9i"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>Black  Dot</td>\n",
       "      <td>We are still in the discovery phase, but so fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-20</td>\n",
       "      <td>Black  Plus</td>\n",
       "      <td>Absolutely love it..  only thing that would ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-07-29</td>\n",
       "      <td>Black  Spot</td>\n",
       "      <td>Works decent, wish it was able to answer more ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>5</td>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>White  Show</td>\n",
       "      <td>Upgraded from an original Echo on Prime Day an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-07-28</td>\n",
       "      <td>Black  Plus</td>\n",
       "      <td>I love everything about it. I love that the ec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating       date    variation  \\\n",
       "2666       5 2018-07-30   Black  Dot   \n",
       "2007       5 2018-07-20  Black  Plus   \n",
       "1142       4 2018-07-29  Black  Spot   \n",
       "1511       5 2018-07-30  White  Show   \n",
       "1828       4 2018-07-28  Black  Plus   \n",
       "\n",
       "                                       verified_reviews  feedback  \n",
       "2666  We are still in the discovery phase, but so fa...         1  \n",
       "2007  Absolutely love it..  only thing that would ma...         1  \n",
       "1142  Works decent, wish it was able to answer more ...         1  \n",
       "1511  Upgraded from an original Echo on Prime Day an...         1  \n",
       "1828  I love everything about it. I love that the ec...         1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BONUS: predict the rating\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "CiyyP7T6Bykf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    2286\n",
       "4     455\n",
       "1     161\n",
       "3     152\n",
       "2      96\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "BNNejlJgB2aY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7257"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base rate\n",
    "round(df.rating.value_counts()[5] / len(df), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "8ZE1JqorB9Lb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      "[[  2   0   0   4  31]\n",
      " [  1   1   0   2  10]\n",
      " [  0   0   0   3  32]\n",
      " [  0   0   0   8  84]\n",
      " [  0   0   0   5 447]]\n",
      "ACCURACY SCORE:\n",
      "0.7270\n"
     ]
    }
   ],
   "source": [
    "# Select features\n",
    "X = df['verified_reviews'] # the features we want to analyze\n",
    "y = df['rating'] # the labels, or answers, we want to test against\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Define classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Generate Model on training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation - test set\n",
    "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "EfE1iXi0Eeuz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucku\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      "[[  7   0   0   3  27]\n",
      " [  0   4   0   3   7]\n",
      " [  0   0  12   4  19]\n",
      " [  0   0   0  34  58]\n",
      " [  0   0   1   1 450]]\n",
      "ACCURACY SCORE:\n",
      "0.8048\n"
     ]
    }
   ],
   "source": [
    "# BONUS 2: use random forest\n",
    "\n",
    "# Define classifier\n",
    "classifier = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Generate Model on training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation - test set\n",
    "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFt-MD98E1E4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "h0VsVUPLT4Tn",
    "RXlVy05IUC-D",
    "1QZOutDwsYij"
   ],
   "include_colab_link": true,
   "name": "Text_Analytics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
